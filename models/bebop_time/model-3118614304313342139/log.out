dataset path: /home/dharmesh/Documents/neurocontroller-hotm/datasets_and_models/bebop_time/dataset.npy
output indices: [6, 7]
learning rule: adam
learning rate: 0.001
minibatch size: 512
hid layer dims: [50, 50, 50, 50, 50]

Train on 9440000 samples, validate on 1180000 samples
Epoch 1/500
 - 143s - loss: 0.0940 - mean_absolute_error: 0.1047 - val_loss: 0.0373 - val_mean_absolute_error: 0.0500
Epoch 2/500
 - 139s - loss: 0.0340 - mean_absolute_error: 0.0463 - val_loss: 0.0276 - val_mean_absolute_error: 0.0402
Epoch 3/500
 - 139s - loss: 0.0259 - mean_absolute_error: 0.0373 - val_loss: 0.0311 - val_mean_absolute_error: 0.0435
Epoch 4/500
 - 140s - loss: 0.0224 - mean_absolute_error: 0.0331 - val_loss: 0.0208 - val_mean_absolute_error: 0.0322
Epoch 5/500
 - 138s - loss: 0.0203 - mean_absolute_error: 0.0308 - val_loss: 0.0183 - val_mean_absolute_error: 0.0269
Epoch 6/500
 - 139s - loss: 0.0188 - mean_absolute_error: 0.0292 - val_loss: 0.0164 - val_mean_absolute_error: 0.0266
Epoch 7/500
 - 140s - loss: 0.0179 - mean_absolute_error: 0.0283 - val_loss: 0.0173 - val_mean_absolute_error: 0.0288
Epoch 8/500
 - 140s - loss: 0.0171 - mean_absolute_error: 0.0273 - val_loss: 0.0169 - val_mean_absolute_error: 0.0274
Epoch 9/500
 - 140s - loss: 0.0166 - mean_absolute_error: 0.0266 - val_loss: 0.0159 - val_mean_absolute_error: 0.0245
Epoch 10/500
 - 140s - loss: 0.0162 - mean_absolute_error: 0.0260 - val_loss: 0.0153 - val_mean_absolute_error: 0.0256
Epoch 11/500
 - 140s - loss: 0.0159 - mean_absolute_error: 0.0255 - val_loss: 0.0179 - val_mean_absolute_error: 0.0267
Epoch 12/500
 - 139s - loss: 0.0155 - mean_absolute_error: 0.0250 - val_loss: 0.0152 - val_mean_absolute_error: 0.0249

Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 13/500
 - 140s - loss: 0.0137 - mean_absolute_error: 0.0229 - val_loss: 0.0134 - val_mean_absolute_error: 0.0222
Epoch 14/500
 - 141s - loss: 0.0136 - mean_absolute_error: 0.0226 - val_loss: 0.0139 - val_mean_absolute_error: 0.0230
Epoch 15/500
 - 139s - loss: 0.0135 - mean_absolute_error: 0.0224 - val_loss: 0.0138 - val_mean_absolute_error: 0.0218
Epoch 16/500
 - 140s - loss: 0.0134 - mean_absolute_error: 0.0223 - val_loss: 0.0137 - val_mean_absolute_error: 0.0222
Epoch 17/500
 - 140s - loss: 0.0134 - mean_absolute_error: 0.0222 - val_loss: 0.0139 - val_mean_absolute_error: 0.0225
Epoch 18/500
 - 142s - loss: 0.0133 - mean_absolute_error: 0.0220 - val_loss: 0.0142 - val_mean_absolute_error: 0.0225

Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 19/500
 - 140s - loss: 0.0125 - mean_absolute_error: 0.0211 - val_loss: 0.0124 - val_mean_absolute_error: 0.0205
Epoch 20/500
 - 140s - loss: 0.0125 - mean_absolute_error: 0.0209 - val_loss: 0.0133 - val_mean_absolute_error: 0.0224
Epoch 21/500
 - 140s - loss: 0.0125 - mean_absolute_error: 0.0209 - val_loss: 0.0125 - val_mean_absolute_error: 0.0211
Epoch 22/500
 - 140s - loss: 0.0124 - mean_absolute_error: 0.0208 - val_loss: 0.0126 - val_mean_absolute_error: 0.0209

Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 23/500
 - 142s - loss: 0.0120 - mean_absolute_error: 0.0203 - val_loss: 0.0121 - val_mean_absolute_error: 0.0200
Epoch 24/500
 - 140s - loss: 0.0120 - mean_absolute_error: 0.0203 - val_loss: 0.0121 - val_mean_absolute_error: 0.0203
Epoch 25/500
 - 138s - loss: 0.0120 - mean_absolute_error: 0.0202 - val_loss: 0.0122 - val_mean_absolute_error: 0.0204
Epoch 26/500
 - 138s - loss: 0.0120 - mean_absolute_error: 0.0202 - val_loss: 0.0121 - val_mean_absolute_error: 0.0204

Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 27/500
 - 140s - loss: 0.0118 - mean_absolute_error: 0.0199 - val_loss: 0.0120 - val_mean_absolute_error: 0.0201
Epoch 28/500
 - 139s - loss: 0.0118 - mean_absolute_error: 0.0199 - val_loss: 0.0121 - val_mean_absolute_error: 0.0200
Epoch 00028: early stopping
