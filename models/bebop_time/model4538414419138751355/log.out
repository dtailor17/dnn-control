dataset path: /home/dharmesh/Documents/neurocontroller-hotm/datasets_and_models/bebop_time/dataset.npy
output indices: [6, 7]
learning rule: adam
learning rate: 0.001
minibatch size: 512
hid layer dims: [50, 50, 50, 50]

Train on 9440000 samples, validate on 1180000 samples
Epoch 1/500
 - 130s - loss: 0.0981 - mean_absolute_error: 0.1106 - val_loss: 0.0459 - val_mean_absolute_error: 0.0570
Epoch 2/500
 - 125s - loss: 0.0372 - mean_absolute_error: 0.0491 - val_loss: 0.0383 - val_mean_absolute_error: 0.0448
Epoch 3/500
 - 125s - loss: 0.0288 - mean_absolute_error: 0.0396 - val_loss: 0.0244 - val_mean_absolute_error: 0.0356
Epoch 4/500
 - 126s - loss: 0.0243 - mean_absolute_error: 0.0353 - val_loss: 0.0236 - val_mean_absolute_error: 0.0324
Epoch 5/500
 - 125s - loss: 0.0220 - mean_absolute_error: 0.0329 - val_loss: 0.0232 - val_mean_absolute_error: 0.0344
Epoch 6/500
 - 126s - loss: 0.0205 - mean_absolute_error: 0.0314 - val_loss: 0.0182 - val_mean_absolute_error: 0.0299
Epoch 7/500
 - 126s - loss: 0.0192 - mean_absolute_error: 0.0302 - val_loss: 0.0184 - val_mean_absolute_error: 0.0296
Epoch 8/500
 - 125s - loss: 0.0185 - mean_absolute_error: 0.0293 - val_loss: 0.0173 - val_mean_absolute_error: 0.0281
Epoch 9/500
 - 126s - loss: 0.0178 - mean_absolute_error: 0.0286 - val_loss: 0.0189 - val_mean_absolute_error: 0.0292
Epoch 10/500
 - 125s - loss: 0.0173 - mean_absolute_error: 0.0281 - val_loss: 0.0164 - val_mean_absolute_error: 0.0275
Epoch 11/500
 - 126s - loss: 0.0169 - mean_absolute_error: 0.0277 - val_loss: 0.0154 - val_mean_absolute_error: 0.0263
Epoch 12/500
 - 125s - loss: 0.0165 - mean_absolute_error: 0.0273 - val_loss: 0.0161 - val_mean_absolute_error: 0.0269
Epoch 13/500
 - 126s - loss: 0.0163 - mean_absolute_error: 0.0270 - val_loss: 0.0158 - val_mean_absolute_error: 0.0271
Epoch 14/500
 - 125s - loss: 0.0160 - mean_absolute_error: 0.0267 - val_loss: 0.0154 - val_mean_absolute_error: 0.0254
Epoch 15/500
 - 125s - loss: 0.0158 - mean_absolute_error: 0.0264 - val_loss: 0.0162 - val_mean_absolute_error: 0.0282
Epoch 16/500
 - 125s - loss: 0.0156 - mean_absolute_error: 0.0262 - val_loss: 0.0160 - val_mean_absolute_error: 0.0267
Epoch 17/500
 - 127s - loss: 0.0154 - mean_absolute_error: 0.0259 - val_loss: 0.0154 - val_mean_absolute_error: 0.0260

Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 18/500
 - 125s - loss: 0.0139 - mean_absolute_error: 0.0242 - val_loss: 0.0138 - val_mean_absolute_error: 0.0237
Epoch 19/500
 - 126s - loss: 0.0138 - mean_absolute_error: 0.0240 - val_loss: 0.0134 - val_mean_absolute_error: 0.0232
Epoch 20/500
 - 126s - loss: 0.0137 - mean_absolute_error: 0.0238 - val_loss: 0.0145 - val_mean_absolute_error: 0.0242
Epoch 21/500
 - 125s - loss: 0.0136 - mean_absolute_error: 0.0237 - val_loss: 0.0136 - val_mean_absolute_error: 0.0236
Epoch 22/500
 - 126s - loss: 0.0136 - mean_absolute_error: 0.0236 - val_loss: 0.0147 - val_mean_absolute_error: 0.0261

Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 23/500
 - 125s - loss: 0.0129 - mean_absolute_error: 0.0228 - val_loss: 0.0129 - val_mean_absolute_error: 0.0229
Epoch 24/500
 - 125s - loss: 0.0129 - mean_absolute_error: 0.0227 - val_loss: 0.0129 - val_mean_absolute_error: 0.0222
Epoch 25/500
 - 126s - loss: 0.0128 - mean_absolute_error: 0.0227 - val_loss: 0.0132 - val_mean_absolute_error: 0.0224
Epoch 26/500
 - 125s - loss: 0.0128 - mean_absolute_error: 0.0226 - val_loss: 0.0131 - val_mean_absolute_error: 0.0234
Epoch 27/500
 - 126s - loss: 0.0128 - mean_absolute_error: 0.0225 - val_loss: 0.0130 - val_mean_absolute_error: 0.0232

Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 28/500
 - 125s - loss: 0.0125 - mean_absolute_error: 0.0221 - val_loss: 0.0127 - val_mean_absolute_error: 0.0220
Epoch 29/500
 - 125s - loss: 0.0124 - mean_absolute_error: 0.0221 - val_loss: 0.0127 - val_mean_absolute_error: 0.0224
Epoch 30/500
 - 126s - loss: 0.0124 - mean_absolute_error: 0.0221 - val_loss: 0.0127 - val_mean_absolute_error: 0.0221
Epoch 31/500
 - 126s - loss: 0.0124 - mean_absolute_error: 0.0220 - val_loss: 0.0127 - val_mean_absolute_error: 0.0219
Epoch 32/500
 - 125s - loss: 0.0124 - mean_absolute_error: 0.0220 - val_loss: 0.0126 - val_mean_absolute_error: 0.0220
Epoch 33/500
 - 126s - loss: 0.0124 - mean_absolute_error: 0.0220 - val_loss: 0.0126 - val_mean_absolute_error: 0.0218
Epoch 34/500
 - 126s - loss: 0.0124 - mean_absolute_error: 0.0220 - val_loss: 0.0126 - val_mean_absolute_error: 0.0222
Epoch 35/500
 - 125s - loss: 0.0124 - mean_absolute_error: 0.0219 - val_loss: 0.0128 - val_mean_absolute_error: 0.0226
Epoch 36/500
 - 126s - loss: 0.0124 - mean_absolute_error: 0.0219 - val_loss: 0.0127 - val_mean_absolute_error: 0.0222

Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 37/500
 - 126s - loss: 0.0122 - mean_absolute_error: 0.0217 - val_loss: 0.0125 - val_mean_absolute_error: 0.0222
Epoch 38/500
 - 126s - loss: 0.0122 - mean_absolute_error: 0.0217 - val_loss: 0.0125 - val_mean_absolute_error: 0.0219
Epoch 00038: early stopping
